import itertools
import random
import json
import os

import numpy as np
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, Dot, Reshape, Dense
from tensorflow.keras.optimizers import Adam

# List of co-occurent apis
api_list = []

def get_api_list(file_path):
    with open(file_path, 'r') as file:
        data = json.load(file)

    # Dữ liệu được lấy từ khóa 'data' trong đối tượng JSON
    api_data = data['data']
    
    for method, apis in api_data.items():
        api_list.append(apis)

def find_json_files(root_dir):
    json_files = []
    for dirpath, dirnames, filenames in os.walk(root_dir):
        for file in filenames:
            if file.endswith('.json'):
                full_path = os.path.join(dirpath, file)
                json_files.append(full_path)
    return json_files


if __name__ == '__main__':

    json_paths = find_json_files('./ds_json')
    
    for json_path in json_paths:
        get_api_list(json_path)

    # Thu thập tất cả các API duy nhất
    unique_apis = set(itertools.chain.from_iterable(api_list))

    print("Vocabulary length:")
    print(len(unique_apis))

    # Tạo từ điển từ các API duy nhất
    api_to_index = {api: idx for idx, api in enumerate(unique_apis)}
    index_to_api = {idx: api for api, idx in api_to_index.items()}


    # Tạo positive và negative samples
    positive_pairs = []
    all_apis = list(api_to_index.keys())


    k = 1  # Đặt k là khoảng cách tối đa giữa các từ trong cùng một cặp
    for apis in api_list:
        n = len(apis)
        for i in range(n):
            # Tạo cặp từ từ đích hiện tại đến tất cả các từ trong phạm vi k
            for j in range(max(0, i-k), min(n, i+k+1)):
                if i != j:  # Đảm bảo rằng không tạo cặp từ với chính nó
                    positive_pairs.append((api_to_index[apis[i]], api_to_index[apis[j]], 1))

    print(positive_pairs[:10])  # In ra 10 cặp đầu tiên để kiểm tra

    positive_pairs_set = set(positive_pairs)  # Chuyển positive pairs vào một set cho tra cứu nhanh
    negative_pairs = []

    print("Num of positive pairs: ", len(positive_pairs))

    target_negative_count = len(positive_pairs) // 1  # Ajust the ratio according to your memory limits

    while len(negative_pairs) < target_negative_count:
        print("Generate negative pairs: ", len(negative_pairs), " / ", target_negative_count)
        api1, api2 = random.sample(all_apis, 2)
        pair = (api_to_index[api1], api_to_index[api2])
        
        # Kiểm tra để đảm bảo rằng pair này không phải là positive pair
        if pair not in positive_pairs_set and (pair[0], pair[1], 0) not in negative_pairs:
            negative_pairs.append((pair[0], pair[1], 0))

    # Gộp và xáo trộn cặp
    all_pairs = positive_pairs + negative_pairs
    random.shuffle(all_pairs)

    # Tách thành features và labels
    pairs, labels = zip(*[(pair[:2], pair[2]) for pair in all_pairs])


    # Thiết lập tham số
    vocab_size = len(api_to_index)
    embedding_dim = 100

    # Định nghĩa mô hình
    input_target = Input((1,))
    input_context = Input((1,))

    embedding = Embedding(vocab_size, embedding_dim, input_length=1, name='embedding_layer')
    target = embedding(input_target)
    target = Reshape((embedding_dim,))(target)
    context = embedding(input_context)
    context = Reshape((embedding_dim,))(context)

    dot_product = Dot(axes=1)([target, context])
    output = Dense(1, activation='sigmoid')(dot_product)

    model = Model(inputs=[input_target, input_context], outputs=output)

    # Biên dịch mô hình
    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])

    # Huấn luyện mô hình
    pairs = np.array(pairs)
    labels = np.array(labels)
    model.fit([pairs[:, 0], pairs[:, 1]], labels, epochs=10, batch_size=64)

    # Save model
    model.save("./w2v.h5")

    word = ''

    # Kiểm tra xem từ có trong từ điển không
    if word in api_to_index:
        # Lấy chỉ số của từ
        word_idx = api_to_index[word]

        # Truy xuất lớp nhúng và lấy trọng số
        embedding_layer = model.get_layer('embedding_layer')
        embeddings = embedding_layer.get_weights()[0]

        # Lấy vector nhúng cho từ
        word_embedding = embeddings[word_idx]

        # In vector nhúng
        print(f"Embedding vector for '{word}': {word_embedding}")
    else:
        print(f"Word '{word}' not found in the vocabulary.")