import pickle
import os
import json
import random
import networkx as nx
import numpy as np
from scipy.spatial import distance
import tensorflow as tf
from tensorflow.keras import layers, Model
from tensorflow.keras.optimizers import Adam
from sklearn.preprocessing import LabelBinarizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

# Load các đối tượng
with open('api_mappings.pkl', 'rb') as f:
    api_to_index = pickle.load(f)
    index_to_api = pickle.load(f)

# Load mô hình Word2Vec đã huấn luyện
model = tf.keras.models.load_model('w2v.h5')

def api2embed(api):
    # Lấy chỉ số của từ
    word_idx = api_to_index[api]

    # Truy xuất lớp nhúng và lấy trọng số
    embedding_layer = model.get_layer('embedding_layer')
    embeddings = embedding_layer.get_weights()[0]

    # Lấy vector nhúng cho từ
    word_embedding = embeddings[word_idx]

    return word_embedding

# Đọc dữ liệu từ thư mục JSON
apk_data = []
labels = []

for root, _, files in os.walk('./ds_json'):
    for file in files:
        if file.endswith('.json'):
            with open(os.path.join(root, file), 'r') as f:
                data = json.load(f)
                data['filename'] = file
                apk_data.append(data)
                labels.append(data['class'])

# Chọn ngẫu nhiên 1000 API làm đỉnh (Vertex)
selected_apis = random.sample(list(api_to_index.keys()), 1000)
selected_api_embeddings = {api: api2embed(api) for api in selected_apis}

# Tạo đồ thị
G = nx.Graph()

# Thêm các nút là file APK và các API được chọn
for apk in apk_data:
    G.add_node(apk['filename'], type='apk')

for api in selected_api_embeddings.keys():
    G.add_node(api, type='api', embedding=selected_api_embeddings[api])

# Thêm các cạnh giữa các file APK và các API mà chúng gọi
for apk in apk_data:
    apk_node = apk['filename']
    for apis in apk['data']:
        for api in apis:
            if api in selected_apis:
                G.add_edge(apk_node, api)

# Tính toán giá trị trung bình của các vector embedding của các API được gọi bởi mỗi APK
for apk in apk_data:
    apk_node = apk['filename']
    neighbors = list(G.neighbors(apk_node))
    neighbor_embeddings = [G.nodes[neighbor]['embedding'] for neighbor in neighbors if 'embedding' in G.nodes[neighbor]]
    if neighbor_embeddings:
        G.nodes[apk_node]['embedding'] = np.mean(neighbor_embeddings, axis=0)

# Thêm các cạnh giữa các API dựa trên embedding vector
for i, api in enumerate(selected_apis):
    api_vector = selected_api_embeddings[api]
    distances = []
    for j, other_api in enumerate(selected_apis):
        if i != j:
            dist = distance.euclidean(api_vector, selected_api_embeddings[other_api])
            distances.append((dist, other_api))
    distances.sort()
    for _, closest_api in distances[:100]:  # Lấy 100 API gần nhất
        G.add_edge(api, closest_api)

# Chuyển đổi đồ thị NetworkX sang ma trận adjacency và ma trận đặc trưng
nodes = list(G.nodes)
node_features = np.array([G.nodes[node].get('embedding', np.zeros(100)) for node in nodes])

# Chỉ lấy các nút ứng dụng (app nodes) để tạo nhãn
app_nodes = [apk['filename'] for apk in apk_data]
app_labels = [labels[i] for i, apk in enumerate(apk_data) if apk['filename'] in app_nodes]

# Chuyển đổi nhãn thành one-hot encoding
label_binarizer = LabelBinarizer()
app_labels = label_binarizer.fit_transform(app_labels)

# Tạo ma trận adjacency
adjacency_matrix = nx.adjacency_matrix(G, nodelist=nodes).todense()

# Lấy chỉ số của các nút ứng dụng
app_indices = [nodes.index(app) for app in app_nodes]

# Chia dữ liệu thành train/test bằng cách tạo mask
train_indices, test_indices = train_test_split(app_indices, test_size=0.3, stratify=app_labels)

# Tạo mask cho các nút train và test
train_mask = np.zeros(len(nodes), dtype=bool)
train_mask[train_indices] = True
test_mask = np.zeros(len(nodes), dtype=bool)
test_mask[test_indices] = True

# Pad labels để khớp với kích thước của node_features và adjacency_matrix
padded_labels = np.zeros((len(nodes), app_labels.shape[1]))
padded_labels[app_indices, :] = app_labels

class GraphConvolutionLayer(layers.Layer):
    def __init__(self, output_dim, **kwargs):
        super(GraphConvolutionLayer, self).__init__(**kwargs)
        self.output_dim = output_dim

    def build(self, input_shape):
        feature_shape = input_shape[0]  # Lấy hình dạng của đầu vào features
        self.kernel = self.add_weight(shape=(feature_shape[-1], self.output_dim),
                                      initializer='glorot_uniform',
                                      trainable=True)

    def call(self, inputs):
        features, adjacency_matrix = inputs
        aggregated_features = tf.matmul(adjacency_matrix, features)
        return tf.matmul(aggregated_features, self.kernel)



# Định nghĩa mô hình
input_features = layers.Input(shape=(node_features.shape[1],))
input_adjacency = layers.Input(shape=(len(nodes),))

gc1 = GraphConvolutionLayer(16)([input_features, input_adjacency])

gc1 = layers.ReLU()(gc1)
gc2 = GraphConvolutionLayer(app_labels.shape[1])([gc1, input_adjacency])
output = layers.Softmax()(gc2)

model = Model(inputs=[input_features, input_adjacency], outputs=output)
model.compile(optimizer=Adam(learning_rate=0.01), loss='categorical_crossentropy', metrics=['accuracy'])

# Huấn luyện mô hình
history = model.fit(
    [node_features, adjacency_matrix],
    padded_labels,
    sample_weight=train_mask,
    epochs=1000,
    batch_size=len(nodes),
    verbose=2
)

predictions = model.predict([node_features, adjacency_matrix], batch_size=len(nodes))
predicted_labels = np.argmax(predictions, axis=1)
true_labels = np.argmax(padded_labels, axis=1)

test_predictions = predicted_labels[test_indices]
test_true_labels = true_labels[test_indices]

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

accuracy = accuracy_score(test_true_labels, test_predictions)
precision = precision_score(test_true_labels, test_predictions, average='weighted')
recall = recall_score(test_true_labels, test_predictions, average='weighted')
f1 = f1_score(test_true_labels, test_predictions, average='weighted')

print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1 Score: {f1:.4f}")

print(classification_report(test_true_labels, test_predictions, target_names=label_binarizer.classes_))

test_loss, test_acc = model.evaluate([node_features, adjacency_matrix], padded_labels, sample_weight=test_mask.astype(float), batch_size=len(nodes))
print(f"Test Loss: {test_loss}, Test Accuracy: {test_acc}")
